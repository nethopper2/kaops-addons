metadata:
  slug: 'open-webui'
  name: 'Open WebUI'
  description: 'Open WebUI is a self-hosted, offline AI platform supporting LLM runners like Ollama.'
  logoUrlDark: 'https://avatars.githubusercontent.com/u/158137808?s=200&v=4' # TODO
  logoUrlLight: 'https://avatars.githubusercontent.com/u/158137808?s=200&v=4' # TODO
  detailUrl: 'https://openwebui.com/'
  externalUrl: 'https://openwebui.com/'
  installTargets:
    hub: true
    hubRequired: true
    edge: false

forms: 
  - slug: default
    label: 'Open WebUI configuration'
    description: 'Something cool to say about it, maybe'
    steps:
      - slug: 'models'
        variableSlugs:
        - 'modelsToPull'
        - 'gpuEnabled'
        ui:
          label: 'Models'  
variables:
  - slug: 'modelsToPull'
    name: 'pull'
    path: 'open-webui.ollama.ollama.models.pull'
    forms: ['default']
    ui:
      # TODO: change text to list
      type: 'text'
      label: 'LLM Models to Pull'
      placeholder: "llama3.3"
      help: 'Models to pull from: https://ollama.com/search'
      validation: 'required'
  - slug: 'gpuEnabled'
    name: 'gpu'
    path: 'open-webui.ollama.ollama.gpu'
    forms: ['default']
    ui:
      type: 'toggle'
      label: 'Use GPU'
system:
  - path: 'open-webui.ingress.host'
    value: 'chat.${system.hubCluster.ingressHostname}'
    